diff --git a/vllm_mindspore/__init__.py b/vllm_mindspore/__init__.py
index e866888..e7e3267 100644
--- a/vllm_mindspore/__init__.py
+++ b/vllm_mindspore/__init__.py
@@ -465,6 +465,7 @@ Processor._validate_structured_output = v1_process_validate_structured_output
 
 from vllm_mindspore.multimodal.processing import call_hf_processor
 from vllm.multimodal.processing import InputProcessingContext
+
 InputProcessingContext.call_hf_processor = call_hf_processor
 
 check_ready()
diff --git a/vllm_mindspore/model_executor/layers/rotary_embedding.py b/vllm_mindspore/model_executor/layers/rotary_embedding.py
index 2a8364a..5b53186 100644
--- a/vllm_mindspore/model_executor/layers/rotary_embedding.py
+++ b/vllm_mindspore/model_executor/layers/rotary_embedding.py
@@ -28,27 +28,27 @@ import itertools
 import math
 from typing import Any, Optional, Union
 
-import numpy as np
 import mindspore as ms
+import numpy as np
 from mindspore import Tensor, mint, nn, ops
 from mindspore.common import dtype as mstype
 from mindspore.ops.auto_generate.gen_ops_prim import SliceExt
 from transformers import PretrainedConfig
 from vllm.config import get_current_vllm_config
 
-from vllm_mindspore.model_executor.utils import get_model_context
 from vllm_mindspore.model_executor.models.vision import (
-    get_llm_pos_ids_for_vision
-)
+    get_llm_pos_ids_for_vision)
+from vllm_mindspore.model_executor.utils import get_model_context
+
 
 def _get_feat_extract_output_lengths(input_lengths: ms.Tensor):
     input_lengths_leave = input_lengths % 100
     feat_lengths = (input_lengths_leave - 1) // 2 + 1
-    output_lengths = (
-        ((feat_lengths - 1) // 2 + 1 - 1) // 2 + 1 + (input_lengths // 100) * 13
-    )
+    output_lengths = (((feat_lengths - 1) // 2 + 1 - 1) // 2 + 1 +
+                      (input_lengths // 100) * 13)
     return feat_lengths, output_lengths
 
+
 def _apply_rotary_emb(
     x: Tensor,
     cos: Tensor,
@@ -299,11 +299,13 @@ class MRotaryEmbedding(RotaryEmbedding):
             assert len(self.mrope_section) == 3
             mrope_section_np = np.array(self.mrope_section, dtype=np.int64)
             sec_total = mrope_section_np.sum()
-            h_sec = np.array(list(range(1, self.mrope_section[1] * 3, 3))) + sec_total
-            w_sec = np.array(list(range(2, self.mrope_section[2] * 3, 3))) + 2 * sec_total
+            h_sec = np.array(list(range(1, self.mrope_section[1] * 3,
+                                        3))) + sec_total
+            w_sec = np.array(list(range(2, self.mrope_section[2] * 3,
+                                        3))) + 2 * sec_total
             select_index = np.arange(sec_total, dtype=np.int64)
-            select_index[1 : mrope_section[1] * 3 : 3] = h_sec
-            select_index[2 : mrope_section[2] * 3 : 3] = w_sec
+            select_index[1:mrope_section[1] * 3:3] = h_sec
+            select_index[2:mrope_section[2] * 3:3] = w_sec
             self.rope_select_index = ms.from_numpy(select_index)
         else:
             assert len(self.mrope_section) == 3
@@ -313,14 +315,15 @@ class MRotaryEmbedding(RotaryEmbedding):
             h_sec = np.arange(sec_cu[0], sec_cu[1]) + sec_total
             w_sec = np.arange(sec_cu[1], sec_cu[2]) + 2 * sec_total
             select_index = np.arange(sec_total, dtype=np.int64)
-            select_index[sec_cu[0] : sec_cu[1]] = h_sec
-            select_index[sec_cu[1] : sec_cu[2]] = w_sec
+            select_index[sec_cu[0]:sec_cu[1]] = h_sec
+            select_index[sec_cu[1]:sec_cu[2]] = w_sec
             self.rope_select_index = ms.from_numpy(select_index)
 
         if self.is_neox_style and self.rotary_dim == self.head_size:
             self.rotary_embedding_op = ops.ApplyRotaryPosEmb(2)
 
-    def apply_interleaved_rope(self, x: Tensor, mrope_section: list[int]) -> Tensor:
+    def apply_interleaved_rope(self, x: Tensor,
+                               mrope_section: list[int]) -> Tensor:
         """Apply interleaved MRoPE to 3D rotary embeddings.
         Reorganizes frequency layout from chunked [TTT...HHH...WWW] to
         interleaved [THTHWHTHW...TT], preserving frequency continuity.
@@ -330,7 +333,8 @@ class MRotaryEmbedding(RotaryEmbedding):
         x_t = mint.index_select(x, -1, self.rope_select_index)
         return x_t
 
-    def apply_no_interleaved_rope(self, x: Tensor, mrope_section: list[int]) -> Tensor:
+    def apply_no_interleaved_rope(self, x: Tensor,
+                                  mrope_section: list[int]) -> Tensor:
         """Apply non-interleaved MRoPE to 3D rotary embeddings.
         Reorganizes frequency layout from chunked [TTT...HHH...WWW] to
         non-interleaved [TTTHHHWWW].
@@ -376,7 +380,8 @@ class MRotaryEmbedding(RotaryEmbedding):
         if self.is_neox_style and self.rotary_dim == self.head_size:
             freqs_cos = mint.cat((cos, cos), dim=-1)
             freqs_sin = mint.cat((sin, sin), dim=-1)
-            query, key = self.rotary_embedding_op(query, key, freqs_cos, freqs_sin,
+            query, key = self.rotary_embedding_op(query, key, freqs_cos,
+                                                  freqs_sin,
                                                   batch_valid_length)
             return query, key
 
@@ -493,7 +498,8 @@ class MRotaryEmbedding(RotaryEmbedding):
     ) -> tuple[Tensor, int]:
         """Get mrope input positions and delta value."""
 
-        video_grid_thw = [[1, h, w] for t, h, w in video_grid_thw for _ in range(t)]
+        video_grid_thw = [[1, h, w] for t, h, w in video_grid_thw
+                          for _ in range(t)]
 
         image_token_id = hf_config.image_token_id
         video_token_id = hf_config.video_token_id
@@ -502,8 +508,7 @@ class MRotaryEmbedding(RotaryEmbedding):
 
         input_tokens_tensor = ms.tensor(input_tokens)
         vision_start_indices = ms.ops.argwhere(
-            input_tokens_tensor == vision_start_token_id
-        ).squeeze(1)
+            input_tokens_tensor == vision_start_token_id).squeeze(1)
         vision_tokens = input_tokens_tensor[vision_start_indices + 1]
         image_nums = (vision_tokens == image_token_id).sum()
         video_nums = (vision_tokens == video_token_id).sum()
@@ -548,43 +553,31 @@ class MRotaryEmbedding(RotaryEmbedding):
             )
             text_len = ed - st
 
-            st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+            st_idx = llm_pos_ids_list[-1].max() + 1 if len(
+                llm_pos_ids_list) > 0 else 0
             llm_pos_ids_list.append(
-                mint.arange(text_len).view(1, -1).expand(3, -1) + st_idx
-            )
-
-            t_index = (
-                mint.arange(llm_grid_t)
-                .view(-1, 1)
-                .expand(-1, llm_grid_h * llm_grid_w)
-                .flatten()
-            )
-            h_index = (
-                mint.arange(llm_grid_h)
-                .view(1, -1, 1)
-                .expand(llm_grid_t, -1, llm_grid_w)
-                .flatten()
-            )
-            w_index = (
-                mint.arange(llm_grid_w)
-                .view(1, 1, -1)
-                .expand(llm_grid_t, llm_grid_h, -1)
-                .flatten()
-            )
+                mint.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
+
+            t_index = (mint.arange(llm_grid_t).view(-1, 1).expand(
+                -1, llm_grid_h * llm_grid_w).flatten())
+            h_index = (mint.arange(llm_grid_h).view(1, -1, 1).expand(
+                llm_grid_t, -1, llm_grid_w).flatten())
+            w_index = (mint.arange(llm_grid_w).view(1, 1, -1).expand(
+                llm_grid_t, llm_grid_h, -1).flatten())
             llm_pos_ids_list.append(
-                mint.stack([t_index, h_index, w_index]) + text_len + st_idx
-            )
+                mint.stack([t_index, h_index, w_index]) + text_len + st_idx)
             st = ed + llm_grid_t * llm_grid_h * llm_grid_w
 
         if st < len(input_tokens):
-            st_idx = llm_pos_ids_list[-1].max() + 1 if len(llm_pos_ids_list) > 0 else 0
+            st_idx = llm_pos_ids_list[-1].max() + 1 if len(
+                llm_pos_ids_list) > 0 else 0
             text_len = len(input_tokens) - st
             llm_pos_ids_list.append(
-                mint.arange(text_len).view(1, -1).expand(3, -1) + st_idx
-            )
+                mint.arange(text_len).view(1, -1).expand(3, -1) + st_idx)
 
         llm_positions = mint.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
-        mrope_position_delta = (llm_positions.max() + 1 - len(input_tokens)).item()
+        mrope_position_delta = (llm_positions.max() + 1 -
+                                len(input_tokens)).item()
         llm_positions = llm_positions[:, context_len:seq_len]
         return llm_positions, mrope_position_delta
 
@@ -636,8 +629,8 @@ class MRotaryEmbedding(RotaryEmbedding):
             video_frame_num = 1
             mm_data_idx = 0
             for modality_type, start_idx, end_idx in input_type_group:
-                st_idx = int(llm_pos_ids_list[-1].max() + 1) if len(
-                    llm_pos_ids_list) > 0 else 0
+                st_idx = int(llm_pos_ids_list[-1].max() +
+                             1) if len(llm_pos_ids_list) > 0 else 0
                 if modality_type == "image":
                     t, h, w = (
                         image_grid_thw[mm_data_idx][0],
@@ -652,14 +645,14 @@ class MRotaryEmbedding(RotaryEmbedding):
                         np.arange(llm_grid_t, dtype=np.int64),
                         np.arange(llm_grid_h, dtype=np.int64),
                         np.arange(llm_grid_w, dtype=np.int64),
-                        indexing='ij'
-                    )
+                        indexing='ij')
 
                     stacked = np.stack([
                         t_indices.ravel(),
                         h_indices.ravel(),
                         w_indices.ravel()
-                    ], axis=0) + st_idx
+                    ],
+                                       axis=0) + st_idx
 
                     llm_pos_ids_list.append(stacked)
                     mm_data_idx += 1
@@ -679,14 +672,14 @@ class MRotaryEmbedding(RotaryEmbedding):
                             np.arange(t_idx, dtype=np.int64),
                             np.arange(llm_grid_h, dtype=np.int64),
                             np.arange(llm_grid_w, dtype=np.int64),
-                            indexing='ij'
-                        )
+                            indexing='ij')
 
                         stacked = np.stack([
                             t_indices.ravel(),
                             h_indices.ravel(),
                             w_indices.ravel()
-                        ], axis=0) + st_idx
+                        ],
+                                           axis=0) + st_idx
 
                         llm_pos_ids_list.append(stacked)
 
@@ -730,20 +723,18 @@ class MRotaryEmbedding(RotaryEmbedding):
             video_grid_thw = Tensor(video_grid_thw)
         input_ids = Tensor(input_tokens)
         if input_ids is None or input_ids.ndim != 1:
-            raise ValueError("_omni3_get_input_positions_tensor expects 1D input_ids")
+            raise ValueError(
+                "_omni3_get_input_positions_tensor expects 1D input_ids")
 
         seq_len = input_ids.shape[0]
         if audio_feature_lengths is not None and not isinstance(
-            audio_feature_lengths, Tensor
-        ):
-            audio_feature_lengths = Tensor(
-                audio_feature_lengths, dtype=ms.int64
-            )
+                audio_feature_lengths, Tensor):
+            audio_feature_lengths = Tensor(audio_feature_lengths,
+                                           dtype=ms.int64)
         if second_per_grid_ts is None:
             if video_grid_thw is not None and video_grid_thw.numel() > 0:
-                second_per_grids = mint.ones(
-                    video_grid_thw.shape[0], dtype=ms.float32
-                )
+                second_per_grids = mint.ones(video_grid_thw.shape[0],
+                                             dtype=ms.float32)
             else:
                 second_per_grids = Tensor([], dtype=ms.float32)
         else:
@@ -758,19 +749,16 @@ class MRotaryEmbedding(RotaryEmbedding):
         position_id_per_seconds = config.position_id_per_seconds
 
         vision_start_indices = ops.argwhere(
-            input_ids == vision_start_token_id
-        ).squeeze(1)
+            input_ids == vision_start_token_id).squeeze(1)
         if vision_start_indices.numel() > 0:
             vision_tokens = input_ids[vision_start_indices + 1]
         else:
-            vision_tokens = mint.empty((0,), dtype=input_ids.dtype)
+            vision_tokens = mint.empty((0, ), dtype=input_ids.dtype)
         audio_nums = mint.sum(input_ids == audio_start_token_id)
         image_nums = (vision_tokens == image_token_id).sum()
-        video_nums = (
-            (vision_tokens == audio_start_token_id).sum()
-            if use_audio_in_video
-            else (vision_tokens == video_token_id).sum()
-        )
+        video_nums = ((vision_tokens == audio_start_token_id).sum()
+                      if use_audio_in_video else
+                      (vision_tokens == video_token_id).sum())
 
         llm_pos_ids_list: list[Tensor] = []
         st = 0
@@ -778,17 +766,15 @@ class MRotaryEmbedding(RotaryEmbedding):
         video_idx = 0
         audio_idx = 0
         remain_images, remain_videos, remain_audios = image_nums, video_nums, audio_nums  # noqa: E501
-        multimodal_nums = (
-            image_nums + audio_nums
-            if use_audio_in_video
-            else image_nums + video_nums + audio_nums
-        )  # noqa: E501
+        multimodal_nums = (image_nums +
+                           audio_nums if use_audio_in_video else image_nums +
+                           video_nums + audio_nums)  # noqa: E501
 
         for _ in range(multimodal_nums):
             st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
-            if (image_token_id in input_tokens or video_token_id in input_tokens) and (
-                remain_videos > 0 or remain_images > 0
-            ):
+            if (image_token_id in input_tokens or video_token_id
+                    in input_tokens) and (remain_videos > 0
+                                          or remain_images > 0):
                 ed_vision_start = input_tokens.index(vision_start_token_id, st)
             else:
                 ed_vision_start = len(input_tokens) + 1
@@ -801,200 +787,172 @@ class MRotaryEmbedding(RotaryEmbedding):
             if min_ed == ed_audio_start:
                 text_len = min_ed - st
                 if text_len != 0:
-                    st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    st_idx = llm_pos_ids_list[-1].max(
+                    ) + 1 if llm_pos_ids_list else 0
                     llm_pos_ids_list.append(
-                        mint.arange(text_len, dtype=ms.int64)
-                        .view(1, -1)
-                        .expand(3, -1)
-                        + st_idx
-                    )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                        mint.arange(text_len, dtype=ms.int64).view(
+                            1, -1).expand(3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 bos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 _, audio_len = _get_feat_extract_output_lengths(
-                    audio_feature_lengths[audio_idx]
-                )
-                llm_pos_ids = (
-                    mint.arange(audio_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                    audio_feature_lengths[audio_idx])
+                llm_pos_ids = (mint.arange(audio_len, dtype=ms.int64).view(
+                    1, -1).expand(3, -1) + st_idx)
                 llm_pos_ids_list.append(llm_pos_ids)
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 eos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
                 st += text_len + bos_len + audio_len + eos_len
                 audio_idx += 1
                 remain_audios -= 1
-            elif (
-                min_ed == ed_vision_start
-                and input_ids[ed_vision_start + 1] == image_token_id
-            ):
+            elif (min_ed == ed_vision_start
+                  and input_ids[ed_vision_start + 1] == image_token_id):
                 text_len = min_ed - st
                 if text_len != 0:
-                    st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    st_idx = llm_pos_ids_list[-1].max(
+                    ) + 1 if llm_pos_ids_list else 0
                     llm_pos_ids_list.append(
-                        mint.arange(text_len, dtype=ms.int64)
-                        .view(1, -1)
-                        .expand(3, -1)
-                        + st_idx
-                    )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                        mint.arange(text_len, dtype=ms.int64).view(
+                            1, -1).expand(3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 bos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 grid_t = image_grid_thw[image_idx][0]
                 grid_hs = image_grid_thw[:, 1]
                 grid_ws = image_grid_thw[:, 2]
                 t_index = mint.arange(grid_t.item()) * position_id_per_seconds
                 llm_pos_ids = get_llm_pos_ids_for_vision(
-                    st_idx, image_idx, spatial_merge_size, t_index, grid_hs, grid_ws
-                )
-                image_len = image_grid_thw[image_idx].prod() // (spatial_merge_size**2)
+                    st_idx, image_idx, spatial_merge_size, t_index, grid_hs,
+                    grid_ws)
+                image_len = image_grid_thw[image_idx].prod() // (
+                    spatial_merge_size**2)
                 llm_pos_ids_list.append(llm_pos_ids)
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 eos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
                 st += text_len + bos_len + image_len + eos_len
                 image_idx += 1
                 remain_images -= 1
-            elif (
-                min_ed == ed_vision_start
-                and input_ids[ed_vision_start + 1] == video_token_id
-                and not use_audio_in_video
-            ):
+            elif (min_ed == ed_vision_start
+                  and input_ids[ed_vision_start + 1] == video_token_id
+                  and not use_audio_in_video):
                 text_len = min_ed - st
                 if text_len != 0:
-                    st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    st_idx = llm_pos_ids_list[-1].max(
+                    ) + 1 if llm_pos_ids_list else 0
                     llm_pos_ids_list.append(
-                        mint.arange(text_len, dtype=ms.int64)
-                        .view(1, -1)
-                        .expand(3, -1)
-                        + st_idx
-                    )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                        mint.arange(text_len, dtype=ms.int64).view(
+                            1, -1).expand(3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 bos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 grid_t = video_grid_thw[video_idx][0]
                 grid_hs = video_grid_thw[:, 1]
                 grid_ws = video_grid_thw[:, 2]
-                t_index = (
-                    mint.arange(grid_t.item())
-                    * float(second_per_grids[video_idx].item())
-                    * position_id_per_seconds
-                )
+                t_index = (mint.arange(grid_t.item()) *
+                           float(second_per_grids[video_idx].item()) *
+                           position_id_per_seconds)
                 llm_pos_ids = get_llm_pos_ids_for_vision(
-                    st_idx, video_idx, spatial_merge_size, t_index, grid_hs, grid_ws
-                )
-                video_len = video_grid_thw[video_idx].prod() // (spatial_merge_size**2)
+                    st_idx, video_idx, spatial_merge_size, t_index, grid_hs,
+                    grid_ws)
+                video_len = video_grid_thw[video_idx].prod() // (
+                    spatial_merge_size**2)
                 llm_pos_ids_list.append(llm_pos_ids)
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 eos_len = 1
                 llm_pos_ids_list.append(
-                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(
+                        3, -1) + st_idx)
                 st += text_len + bos_len + video_len + eos_len
                 video_idx += 1
                 remain_videos -= 1
-            elif (
-                min_ed == ed_vision_start
-                and ed_vision_start + 1 == ed_audio_start
-                and use_audio_in_video
-            ):
+            elif (min_ed == ed_vision_start
+                  and ed_vision_start + 1 == ed_audio_start
+                  and use_audio_in_video):
                 text_len = min_ed - st
                 if text_len != 0:
-                    st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                    st_idx = llm_pos_ids_list[-1].max(
+                    ) + 1 if llm_pos_ids_list else 0
                     llm_pos_ids_list.append(
-                        mint.arange(text_len, dtype=ms.int64)
-                        .view(1, -1)
-                        .expand(3, -1)
-                        + st_idx
-                    )
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                        mint.arange(text_len, dtype=ms.int64).view(
+                            1, -1).expand(3, -1) + st_idx)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 bos_len = 1
-                bos_block = (
-                    mint.arange(bos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                bos_block = (mint.arange(bos_len, dtype=ms.int64).view(
+                    1, -1).expand(3, -1) + st_idx)
                 llm_pos_ids_list.append(bos_block)
                 llm_pos_ids_list.append(bos_block)
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 _, audio_len = _get_feat_extract_output_lengths(
-                    audio_feature_lengths[audio_idx]
-                )
-                audio_llm_pos_ids = (
-                    mint.arange(audio_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                    audio_feature_lengths[audio_idx])
+                audio_llm_pos_ids = (mint.arange(
+                    audio_len, dtype=ms.int64).view(1, -1).expand(3, -1) +
+                                     st_idx)
                 grid_t = video_grid_thw[video_idx][0]
                 grid_hs = video_grid_thw[:, 1]
                 grid_ws = video_grid_thw[:, 2]
-                t_index = (
-                    mint.arange(grid_t.item())
-                    * float(second_per_grids[video_idx].item())
-                    * position_id_per_seconds
-                )
+                t_index = (mint.arange(grid_t.item()) *
+                           float(second_per_grids[video_idx].item()) *
+                           position_id_per_seconds)
                 video_llm_pos_ids = get_llm_pos_ids_for_vision(
-                    st_idx, video_idx, spatial_merge_size, t_index, grid_hs, grid_ws
-                )
+                    st_idx, video_idx, spatial_merge_size, t_index, grid_hs,
+                    grid_ws)
                 video_data_index, audio_data_index = 0, 0
-                while (
-                    video_data_index < video_llm_pos_ids.shape[-1]
-                    and audio_data_index < audio_llm_pos_ids.shape[-1]
-                ):
-                    if (
-                        video_llm_pos_ids[0][video_data_index]
-                        <= audio_llm_pos_ids[0][audio_data_index]
-                    ):
+                while (video_data_index < video_llm_pos_ids.shape[-1]
+                       and audio_data_index < audio_llm_pos_ids.shape[-1]):
+                    if (video_llm_pos_ids[0][video_data_index]
+                            <= audio_llm_pos_ids[0][audio_data_index]):
                         llm_pos_ids_list.append(
-                            video_llm_pos_ids[
-                                :, video_data_index : video_data_index + 1
-                            ]
-                        )
+                            video_llm_pos_ids[:, video_data_index:
+                                              video_data_index + 1])
                         video_data_index += 1
                     else:
                         llm_pos_ids_list.append(
-                            audio_llm_pos_ids[
-                                :, audio_data_index : audio_data_index + 1
-                            ]
-                        )
+                            audio_llm_pos_ids[:, audio_data_index:
+                                              audio_data_index + 1])
                         audio_data_index += 1
                 if video_data_index < video_llm_pos_ids.shape[-1]:
                     llm_pos_ids_list.append(
-                        video_llm_pos_ids[
-                            :, video_data_index : video_llm_pos_ids.shape[-1]
-                        ]
-                    )
+                        video_llm_pos_ids[:,
+                                          video_data_index:video_llm_pos_ids.
+                                          shape[-1]])
                 if audio_data_index < audio_llm_pos_ids.shape[-1]:
                     llm_pos_ids_list.append(
-                        audio_llm_pos_ids[
-                            :, audio_data_index : audio_llm_pos_ids.shape[-1]
-                        ]
-                    )
-                video_len = video_grid_thw[video_idx].prod() // (spatial_merge_size**2)
-                st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
+                        audio_llm_pos_ids[:,
+                                          audio_data_index:audio_llm_pos_ids.
+                                          shape[-1]])
+                video_len = video_grid_thw[video_idx].prod() // (
+                    spatial_merge_size**2)
+                st_idx = llm_pos_ids_list[-1].max(
+                ) + 1 if llm_pos_ids_list else 0
                 eos_len = 1
-                eos_block = (
-                    mint.arange(eos_len, dtype=ms.int64).view(1, -1).expand(3, -1)
-                    + st_idx
-                )
+                eos_block = (mint.arange(eos_len, dtype=ms.int64).view(
+                    1, -1).expand(3, -1) + st_idx)
                 llm_pos_ids_list.append(eos_block)
                 llm_pos_ids_list.append(eos_block)
                 st += text_len + bos_len * 2 + audio_len + video_len + eos_len * 2  # noqa: E501
@@ -1007,13 +965,13 @@ class MRotaryEmbedding(RotaryEmbedding):
             st_idx = llm_pos_ids_list[-1].max() + 1 if llm_pos_ids_list else 0
             text_len = len(input_tokens) - st
             llm_pos_ids_list.append(
-                mint.arange(text_len.item(), dtype=ms.int64).view(1, -1).expand(3, -1)
-                + st_idx
-            )
+                mint.arange(text_len.item(), dtype=ms.int64).view(
+                    1, -1).expand(3, -1) + st_idx)
 
         llm_positions = mint.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
         if llm_positions.shape[1] != seq_len:
-            raise RuntimeError("Position ids length mismatch with input ids length")
+            raise RuntimeError(
+                "Position ids length mismatch with input ids length")
 
         mrope_position_delta = llm_positions.max() + 1 - seq_len
         return llm_positions, mrope_position_delta.item()
@@ -1447,7 +1405,8 @@ def get_rope(
                     is_neox_style,
                     dtype,
                     mrope_section=rope_scaling["mrope_section"],
-                    mrope_interleaved=rope_scaling.get("mrope_interleaved", False),
+                    mrope_interleaved=rope_scaling.get("mrope_interleaved",
+                                                       False),
                 )
             else:
                 raise NotImplementedError
diff --git a/vllm_mindspore/model_executor/models/interfaces.py b/vllm_mindspore/model_executor/models/interfaces.py
index 8c23e2b..1573215 100644
--- a/vllm_mindspore/model_executor/models/interfaces.py
+++ b/vllm_mindspore/model_executor/models/interfaces.py
@@ -45,6 +45,7 @@ class SupportsMultiModal(Protocol):
     """
 
     merge_by_field_config: ClassVar[bool] = False
+
     def get_multimodal_embeddings(
             self, **kwargs: object) -> Optional[MultiModalEmbeddings]:
         """
diff --git a/vllm_mindspore/model_executor/models/vision.py b/vllm_mindspore/model_executor/models/vision.py
index 804e231..fb20104 100644
--- a/vllm_mindspore/model_executor/models/vision.py
+++ b/vllm_mindspore/model_executor/models/vision.py
@@ -25,6 +25,7 @@
 
 from mindspore import Tensor, mint
 
+
 def get_llm_pos_ids_for_vision(
     start_idx: int,
     vision_idx: int,
@@ -36,25 +37,15 @@ def get_llm_pos_ids_for_vision(
     llm_pos_ids_list = []
     llm_grid_h = grid_hs[vision_idx].item() // spatial_merge_size
     llm_grid_w = grid_ws[vision_idx].item() // spatial_merge_size
-    h_index = (
-        mint.arange(llm_grid_h)
-        .view(1, -1, 1)
-        .expand(len(t_index), -1, llm_grid_w)
-        .flatten()
-    )
-    w_index = (
-        mint.arange(llm_grid_w)
-        .view(1, 1, -1)
-        .expand(len(t_index), llm_grid_h, -1)
-        .flatten()
-    )
-    t_index_tensor = (
-        Tensor(t_index)
-        .view(-1, 1)
-        .expand(-1, llm_grid_h * llm_grid_w)
-        .long()
-        .flatten()
-    )
+    h_index = (mint.arange(llm_grid_h).view(1, -1,
+                                            1).expand(len(t_index), -1,
+                                                      llm_grid_w).flatten())
+    w_index = (mint.arange(llm_grid_w).view(1, 1,
+                                            -1).expand(len(t_index),
+                                                       llm_grid_h,
+                                                       -1).flatten())
+    t_index_tensor = (Tensor(t_index).view(-1, 1).expand(
+        -1, llm_grid_h * llm_grid_w).long().flatten())
     _llm_pos_ids = mint.stack([t_index_tensor, h_index, w_index])
     llm_pos_ids_list.append(_llm_pos_ids + start_idx)
     llm_pos_ids = mint.cat(llm_pos_ids_list, dim=1)
diff --git a/vllm_mindspore/multimodal/inputs.py b/vllm_mindspore/multimodal/inputs.py
index d8add9d..9d96a95 100644
--- a/vllm_mindspore/multimodal/inputs.py
+++ b/vllm_mindspore/multimodal/inputs.py
@@ -121,6 +121,7 @@ def flat_build_elems(
     field_factory = self._field_factory(modality=modality, key=key)
     return [field_factory(data[cast(slice, s)]) for s in self.slices]
 
+
 @staticmethod
 def _try_stack(nested_tensors: NestedTensors,
                pin_memory: bool = False) -> NestedTensors:
diff --git a/vllm_mindspore/multimodal/processing.py b/vllm_mindspore/multimodal/processing.py
index 3cb7175..1f33a43 100644
--- a/vllm_mindspore/multimodal/processing.py
+++ b/vllm_mindspore/multimodal/processing.py
@@ -16,6 +16,7 @@
 """Adaption for input processor."""
 
 from collections.abc import Mapping
+
 from transformers import BatchFeature, ProcessorMixin
 from vllm.multimodal.processing import InputProcessingContext
 
diff --git a/vllm_mindspore/transformers_patch.py b/vllm_mindspore/transformers_patch.py
index 781fafb..881e8d3 100644
--- a/vllm_mindspore/transformers_patch.py
+++ b/vllm_mindspore/transformers_patch.py
@@ -14,8 +14,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import os
 import importlib
+import os
 import sys
 import types
 import warnings
@@ -23,6 +23,7 @@ import warnings
 os.environ["USE_TORCH"] = "FALSE"
 os.environ["USE_TF"] = "FALSE"
 
+
 def _patch_processing_module():
     import mindone.transformers.models as mo_models
     import transformers.models as tf_models
@@ -61,6 +62,7 @@ def _patch_processing_module():
                     except Exception:
                         continue
 
+
 def patch_transformers():
     try:
         import mindone
diff --git a/vllm_mindspore/v1/serial_utils.py b/vllm_mindspore/v1/serial_utils.py
index f67d32b..6caafa9 100644
--- a/vllm_mindspore/v1/serial_utils.py
+++ b/vllm_mindspore/v1/serial_utils.py
@@ -44,8 +44,8 @@ def _decode_tensor(self, arr: Any) -> ms.Tensor:
 
 
 def _encode_tensor(
-        self, obj: ms.Tensor
-) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:
+        self,
+        obj: ms.Tensor) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:
     assert self.aux_buffers is not None
     # view the tensor as a contiguous 1D array of bytes
     # NOTE: vLLM-MindSpore Plugin:
