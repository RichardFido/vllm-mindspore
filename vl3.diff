diff --git a/vllm_mindspore/transformers_patch.py b/vllm_mindspore/transformers_patch.py
index 881e8d3..a9b11a2 100644
--- a/vllm_mindspore/transformers_patch.py
+++ b/vllm_mindspore/transformers_patch.py
@@ -34,13 +34,14 @@ def _patch_processing_module():
             continue
         try:
             mo_submod = getattr(mo_models, attr)
-            tf_submod = getattr(tf_models, attr, None)
+            _ = getattr(tf_models, attr, None)
         except Exception:
             continue
         if not isinstance(mo_submod, types.ModuleType):
             continue
 
-        # Get all "processing_"* or "*processing*" or "image_processing_"* in mindone
+        # Get all "processing_"* or "*processing*" 
+        # or "image_processing_"* in mindone
         for sub_attr in dir(mo_submod):
             if ("processing" in sub_attr):
                 try:
@@ -56,8 +57,8 @@ def _patch_processing_module():
                 else:
                     # Try to import if not already loaded
                     try:
-                        tf_mod = importlib.import_module(tf_modname)
-                        mo_mod = importlib.import_module(mo_modname)
+                        _ = importlib.import_module(tf_modname)
+                        _ = importlib.import_module(mo_modname)
                         sys.modules[tf_modname] = sys.modules[mo_modname]
                     except Exception:
                         continue
@@ -65,7 +66,7 @@ def _patch_processing_module():
 
 def patch_transformers():
     try:
-        import mindone
+        import mindone  # noqa: F401
     except ImportError:
         warnings.warn("mindone.transformers not installed, "
                       "skip patching transformers.")
